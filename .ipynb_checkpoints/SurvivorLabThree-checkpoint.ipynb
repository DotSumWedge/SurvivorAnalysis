{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c0ec83f",
   "metadata": {},
   "source": [
    "### Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "267edbfd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 21>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LabelEncoder\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataConversionWarning\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m XGBClassifier\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stats\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split, LeaveOneGroupOut\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "from xgboost import XGBClassifier\n",
    "from scipy import stats\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "import math\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3ab5de",
   "metadata": {},
   "source": [
    "### Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6de7c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=DataConversionWarning)\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Get the path to the survivorData directory\n",
    "data_dir = os.path.join(current_dir, '..', 'survivorData2')\n",
    "\n",
    "# List of CSV file names\n",
    "csv_files = [\n",
    "    'advantage_movement.csv',\n",
    "    'boot_mapping.csv',\n",
    "    'castaways.csv',\n",
    "    'castaway_details.csv',\n",
    "    'challenge_description.csv',\n",
    "    'challenge_results.csv',\n",
    "    'confessionals.csv',\n",
    "    'jury_votes.csv',\n",
    "    'screen_time.csv',\n",
    "    'season_palettes.csv',\n",
    "    'season_summary.csv',\n",
    "    'survivor_auction.csv',\n",
    "    'tribe_colours.csv',\n",
    "    'tribe_mapping.csv',\n",
    "    'viewers.csv',\n",
    "    'vote_history.csv'\n",
    "]\n",
    "\n",
    "# Create a dictionary to store the DataFrames\n",
    "dataframes = {}\n",
    "\n",
    "# Loop through each CSV file and read its data into a DataFrame\n",
    "for csv_file in csv_files:\n",
    "    # Specify the relative path to the CSV file\n",
    "    file_path = os.path.join(data_dir, csv_file)\n",
    "    \n",
    "    # Read the data from the CSV file into a pandas DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Store the DataFrame in the dictionary\n",
    "    dataframes[csv_file] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1ac9fa",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470c5e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tribe_status column to category type\n",
    "dataframes['tribe_colours.csv']['tribe_status'] = dataframes['tribe_colours.csv']['tribe_status'].astype('category')\n",
    "\n",
    "# Convert result to a categorical variable\n",
    "dataframes['castaways.csv']['result'] = pd.Categorical(dataframes['castaways.csv']['result'])\n",
    "dataframes['castaways.csv']['result'] = dataframes['castaways.csv']['result'].cat.codes\n",
    "\n",
    "# Merge challenge_results and castaway_details dataframes on castaway_id\n",
    "castawayAll = pd.merge(dataframes['castaways.csv'], dataframes['castaway_details.csv'], on='castaway_id', how =\"left\")\n",
    "\n",
    "# Male = 1, Female = 2, Non-binary = 3\n",
    "castawayAll['genderNumber'] = np.where(castawayAll['gender'] == 'Male', 1,\n",
    "                                     np.where(castawayAll['gender'] == 'Female', 2,\n",
    "                                              np.where(castawayAll['gender'] == 'Non-binary', 3, 0)))\n",
    "\n",
    "# Binary on if the person was the sole survivor\n",
    "castawayAll['won'] = np.where(castawayAll['result'] == 'Sole Survivor', 1, 0)\n",
    "\n",
    "castawayAll = castawayAll.dropna(subset=['age'])\n",
    "\n",
    "# Read the challenge_results.csv file\n",
    "challenge_results = dataframes['challenge_results.csv']\n",
    "\n",
    "# Filter out the rows where challenge_type is \"Immunity\" and the result is \"Won\"\n",
    "immunity_wins = challenge_results[(challenge_results['challenge_type'] == 'Immunity') & (challenge_results['result'] == 'Won')]\n",
    "\n",
    "# Group by 'castaway_id' and count the number of immunity wins\n",
    "immunity_counts = immunity_wins.groupby('castaway_id').size()\n",
    "\n",
    "# Convert Series to DataFrame\n",
    "immunity_counts_df = immunity_counts.reset_index(name='immunityWins')\n",
    "\n",
    "# Set 'castaway_id' as the index for easier merging later\n",
    "immunity_counts_df.set_index('castaway_id', inplace=True)\n",
    "\n",
    "# Set 'castaway_id' as the index for 'castawayAll' dataframe for easier merging\n",
    "castawayAll.set_index('castaway_id', inplace=True)\n",
    "\n",
    "# Merge 'immunity_counts_df' into 'castawayAll'\n",
    "castawayAll = castawayAll.merge(immunity_counts_df, how='left', left_index=True, right_index=True)\n",
    "\n",
    "# Fill NaN values with 0 - assuming that contestants who didn't win any immunity challenges are not present in the immunity_counts_df DataFrame\n",
    "castawayAll['immunityWins'].fillna(0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db39ffe",
   "metadata": {},
   "source": [
    "#### Split DataFrame by Season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eebb9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataframe into a list of dataframes grouped by column name\n",
    "def split_dataframe(df, column_name):\n",
    "    groups = df.groupby(column_name)\n",
    "    result = [group for _, group in groups]\n",
    "    return result\n",
    "\n",
    "# Call split_dataframe on column name \"version_season\"\n",
    "season_split = split_dataframe(castawayAll, 'version_season')\n",
    "\n",
    "# Give each contestant which order they were eliminated from the show\n",
    "for df in season_split:\n",
    "    df['orderOut'] = range(1, len(df) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd8d5c9",
   "metadata": {},
   "source": [
    "### Helper Functions for Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67efa674",
   "metadata": {},
   "source": [
    "Function uses the specified model to make a prediction on the next person to be eliminated from the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7568fe8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def person_prediction(model, remaining_contestants, x_train_current, y_train_current):\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(x_train_current, y_train_current)\n",
    "\n",
    "    # Predict the order for the remaining contestants\n",
    "    x_test_current = remaining_contestants[['age', 'genderNumber', 'immunityWins']]\n",
    "    predicted_order = model.predict(x_test_current)\n",
    "\n",
    "    # Find the contestants with the smallest predicted order\n",
    "    min_predicted_order = np.min(predicted_order)\n",
    "    contestants_with_min_order = np.where(predicted_order == min_predicted_order)[0]\n",
    "\n",
    "    # Randomly select one contestant from those tied with the smallest predicted order\n",
    "    person_predicted = np.random.choice(contestants_with_min_order)\n",
    "\n",
    "    return person_predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06625898",
   "metadata": {},
   "source": [
    "Function uses each model in the models list to make a prediction on the next person to be eliminated from the game.\n",
    "- Returns a list of predictions, one for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246cec2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def models_predict(remaining_contestants, x_train_current, y_train_current, models):\n",
    "    \"\"\"\n",
    "    Use each model in the models list to make a prediction on the next person to be eliminated from the game.\n",
    "    Returns a list of predictions, one for each model.\n",
    "    \"\"\"\n",
    "    \n",
    "    predictions = []\n",
    "    for model_name, model in models:\n",
    "        prediction = person_prediction(model, remaining_contestants, x_train_current, y_train_current)\n",
    "        predictions.append((model_name, prediction))\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fedc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_challenge_features(current_season_version, current_season, current_episode_number, challenge_results):\n",
    "    \"\"\"\n",
    "    Adds data from the current episode/version/season\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filter the challenge results to include only rows up to the current episode, from the current version season, and from the current season\n",
    "    challenge_results_current = challenge_results[(challenge_results['version_season'] == current_season_version) &\n",
    "                                                  (challenge_results['season'] == current_season) &\n",
    "                                                  (challenge_results['episode'] <= current_episode_number)]\n",
    "\n",
    "    # Generate a DataFrame of the number of wins for each castaway\n",
    "    challenge_results_current = challenge_results_current[challenge_results_current['result'] == 'Won']\\\n",
    "                        .groupby('castaway_id')['result'].count().reset_index()\\\n",
    "                        .rename(columns={'result': 'immunityWins'})\n",
    "\n",
    "    # Set 'castaway_id' as the index\n",
    "    challenge_results_current.set_index('castaway_id', inplace=True)\n",
    "\n",
    "    return challenge_results_current\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b9df99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_train_challenge_features(x_train, y_train, x_test, current_season_version, current_season):\n",
    "    \"\"\"\n",
    "    Removes contestants in the testing set from the training set so the model doesn't have data from the future\n",
    "\n",
    "    Todo: include data from previous seasons. Not sure how to figure out which seasons/versions are in the past and which are in the future.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Identify contestants in x_train who are also in the current test set\n",
    "    test_contestants = x_test.index if 'castaway_id' not in x_test.columns else x_test['castaway_id']\n",
    "    \n",
    "    # Remove these contestants from x_train\n",
    "    x_train = x_train[~x_train.index.isin(test_contestants)]\n",
    "    \n",
    "    # Remove these contestants from y_train\n",
    "    y_train = y_train[~y_train.index.isin(test_contestants)]\n",
    "    \n",
    "    return x_train, y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29077f4b",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09e1d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training and testing sets using LeaveOneGroupOut.\n",
    "multilevel_season_splitter = LeaveOneGroupOut()\n",
    "\n",
    "# Create a list of group labels corresponding to each DataFrame in season_split\n",
    "group_labels = [i for i, _ in enumerate(season_split)]\n",
    "\n",
    "models = [\n",
    "    (\"SVM\", svm.SVC(decision_function_shape='ovo')),\n",
    "    (\"Random Forest\", RandomForestClassifier()),\n",
    "    (\"K Neighbors K=1\", KNeighborsClassifier(n_neighbors=1)),\n",
    "    (\"K Neighbors K=3\", KNeighborsClassifier(n_neighbors=3)),\n",
    "    (\"K Neighbors K=5\", KNeighborsClassifier(n_neighbors=5)),\n",
    "    (\"K Neighbors K=7\", KNeighborsClassifier(n_neighbors=7)),\n",
    "    (\"K Neighbors K=9\", KNeighborsClassifier(n_neighbors=9)),\n",
    "    (\"Decision Tree\", DecisionTreeClassifier()),\n",
    "    (\"Naive Bayes\", GaussianNB()),\n",
    "    (\"Gradient Boosting\", GradientBoostingClassifier()),\n",
    "    (\"AdaBoost\", AdaBoostClassifier()),\n",
    "]\n",
    "\n",
    "# Create an empty dictionary to store accuracies\n",
    "accuracies = {model_name: [] for model_name, _ in models}\n",
    "\n",
    "# Create an empty list to store accuracies\n",
    "accuracies_support_vector_machine = []\n",
    "\n",
    "for train_index, test_index in multilevel_season_splitter.split(season_split, groups=group_labels):\n",
    "\n",
    "    x_train = pd.concat([season_split[i][['age', 'genderNumber', 'immunityWins']] for i in train_index])\n",
    "    y_train = pd.concat([season_split[i][['orderOut']] for i in train_index])\n",
    "\n",
    "    x_test = pd.concat([season_split[i][['age', 'genderNumber']] for i in test_index])\n",
    "    y_test = pd.concat([season_split[i][['orderOut']] for i in test_index])\n",
    "\n",
    "    number_starting_contestants = len(x_test)\n",
    "    \n",
    "    correct_elimination_predictions = 0\n",
    "    correct_predictions = {model_name: 0 for model_name, _ in models}\n",
    "    \n",
    "    current_season_version = season_split[test_index[0]]['version_season'].iloc[0]\n",
    "    current_season = group_labels[test_index[0]] + 1\n",
    "    current_episode_number = 1\n",
    "    counter = 0\n",
    "    counter2 = 0\n",
    "    x_train, y_train = fix_train_challenge_features(x_train, y_train, x_test, current_season_version, current_season)\n",
    "\n",
    "    while len(x_test) > 0:\n",
    "        \n",
    "        # Update x_test to include updated challenge results\n",
    "        challenge_features = generate_test_challenge_features(current_season_version, current_season, current_episode_number, challenge_results)\n",
    "\n",
    "        # Drop the 'immunityWins' column from x_test if it exists\n",
    "        if 'immunityWins' in x_test.columns:\n",
    "            x_test.drop('immunityWins', axis=1, inplace=True)\n",
    "        \n",
    "        # Merge challenge_features into x_test\n",
    "        x_test = x_test.merge(challenge_features, how='left', on='castaway_id')\n",
    "        \n",
    "        # Replace NaN values in 'immunityWins' with 0\n",
    "        x_test['immunityWins'].fillna(0, inplace=True)\n",
    "\n",
    "        predictions = models_predict(x_test, x_train, y_train, models) \n",
    "        print(predictions)\n",
    "        \n",
    "        for model_name, prediction_person_index in predictions:\n",
    "            # Check if the prediction is correct\n",
    "            if prediction_person_index == 0:\n",
    "                correct_predictions[model_name] += 1        \n",
    "\n",
    "        # Remove the first element of x_test\n",
    "        x_test = x_test.iloc[1:]\n",
    "        \n",
    "        # Todo: Update how current_episode_number is updated. Contestants are usually eliminated more than once a day so current_episode_number is probably too high\n",
    "        current_episode_number += 1\n",
    "        counter += 1\n",
    "        # If the counter is 3, break the loop\n",
    "        # if counter == 3:\n",
    "        #      break\n",
    "\n",
    "    # Calculate and print the accuracy of the models\n",
    "    for model_name, correct_prediction_count in correct_predictions.items():\n",
    "        # Calculate and print the accuracy of the model\n",
    "        accuracy = (correct_prediction_count / number_starting_contestants) * 100\n",
    "        accuracies[model_name].append(accuracy)\n",
    "\n",
    "    # counter2 += 1\n",
    "    # if counter2 == 3:\n",
    "    # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59ab9731",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'accuracies' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name, accuracy_list \u001b[38;5;129;01min\u001b[39;00m \u001b[43maccuracies\u001b[49m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracies for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy_list\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m     average_accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(accuracy_list) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(accuracy_list)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'accuracies' is not defined"
     ]
    }
   ],
   "source": [
    "for model_name, accuracy_list in accuracies.items():\n",
    "    print(f\"Accuracies for {model_name}: {accuracy_list}\")\n",
    "    average_accuracy = sum(accuracy_list) / len(accuracy_list)\n",
    "    print(f\"Average Accuracy for {model_name}: {average_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0733482f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'accuracies' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Initialize a dictionary to hold average accuracies\u001b[39;00m\n\u001b[0;32m      2\u001b[0m average_accuracies \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name, accuracy_list \u001b[38;5;129;01min\u001b[39;00m \u001b[43maccuracies\u001b[49m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m      5\u001b[0m     average_accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(accuracy_list) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(accuracy_list)\n\u001b[0;32m      6\u001b[0m     average_accuracies[model_name] \u001b[38;5;241m=\u001b[39m average_accuracy\n",
      "\u001b[1;31mNameError\u001b[0m: name 'accuracies' is not defined"
     ]
    }
   ],
   "source": [
    "# Initialize a dictionary to hold average accuracies\n",
    "average_accuracies = {}\n",
    "\n",
    "for model_name, accuracy_list in accuracies.items():\n",
    "    average_accuracy = sum(accuracy_list) / len(accuracy_list)\n",
    "    average_accuracies[model_name] = average_accuracy\n",
    "\n",
    "# Convert the average accuracies dictionary to a pandas DataFrame\n",
    "average_accuracies_df = pd.DataFrame(list(average_accuracies.items()), columns=['Model', 'AverageAccuracy'])\n",
    "\n",
    "# Write the DataFrame to a CSV file\n",
    "average_accuracies_df.to_csv('average_accuracies.csv', index=False)\n",
    "\n",
    "# Similarly, for each model's list of accuracies:\n",
    "for model_name, accuracy_list in accuracies.items():\n",
    "    # Convert the list of accuracies to a pandas DataFrame\n",
    "    accuracies_df = pd.DataFrame(accuracy_list, columns=['Accuracy'])\n",
    "    # Write the DataFrame to a CSV file\n",
    "    accuracies_df.to_csv(f'accuracies_{model_name}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "009e8922",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generate_test_challenge_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 19>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m     expected_result \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[0;32m     14\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcastaway_id\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     15\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimmunityWins\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     16\u001b[0m     })\u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcastaway_id\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     17\u001b[0m     pd\u001b[38;5;241m.\u001b[39mtesting\u001b[38;5;241m.\u001b[39massert_frame_equal(result, expected_result)\n\u001b[1;32m---> 19\u001b[0m \u001b[43mtest_generate_test_challenge_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36mtest_generate_test_challenge_features\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m challenge_results_test \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseason\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m],\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepisode\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mversion_season\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mS01\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mS01\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mS01\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mS01\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mS02\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mS02\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# Add this line\u001b[39;00m\n\u001b[0;32m      9\u001b[0m })\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Test 1: Current season version is 'S01', current season is 1 and current episode is 2\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_test_challenge_features\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mS01\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, challenge_results_test)\n\u001b[0;32m     13\u001b[0m expected_result \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcastaway_id\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimmunityWins\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     16\u001b[0m })\u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcastaway_id\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     17\u001b[0m pd\u001b[38;5;241m.\u001b[39mtesting\u001b[38;5;241m.\u001b[39massert_frame_equal(result, expected_result)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'generate_test_challenge_features' is not defined"
     ]
    }
   ],
   "source": [
    "def test_generate_test_challenge_features():\n",
    "    # Create a DataFrame that mimics the structure of `challenge_results`\n",
    "    challenge_results_test = pd.DataFrame({\n",
    "        'season': [1, 1, 1, 1, 2, 2],\n",
    "        'episode': [1, 1, 2, 2, 1, 1],\n",
    "        'castaway_id': ['A', 'B', 'A', 'B', 'A', 'B'],\n",
    "        'result': ['Won', 'Lost', 'Won', 'Won', 'Won', 'Lost'],\n",
    "        'version_season': ['S01', 'S01', 'S01', 'S01', 'S02', 'S02']  # Add this line\n",
    "    })\n",
    "\n",
    "    # Test 1: Current season version is 'S01', current season is 1 and current episode is 2\n",
    "    result = generate_test_challenge_features('S01', 1, 2, challenge_results_test)\n",
    "    expected_result = pd.DataFrame({\n",
    "        'castaway_id': ['A', 'B'],\n",
    "        'immunityWins': [2, 1]\n",
    "    }).set_index('castaway_id')\n",
    "    pd.testing.assert_frame_equal(result, expected_result)\n",
    "\n",
    "test_generate_test_challenge_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2139ec92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
